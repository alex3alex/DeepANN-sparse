# One lr per layer in the architecture. 
lr = 0.005
act = 'rectifier'
depth = 1 
n_hid = 5000

# One noise type per layer in the architecture. 'binomial_NLP' is salt-and-pepper noise. 'binomial' means simply overwrite with a binomial score.
noise = 'binomial_NLP'
noise_lvl = (0.8,0.005)

weight_regularization_type = 'l2'
weight_regularization_coeff = 0.0
activation_regularization_type = 'l1'
activation_regularization_coeff = 0.0001

# Number of pretraining epochs, one-per-layer
nepochs = 120

# Different validation runs
#        - 100 training examples (x20 different samples of 100 training examples)
#        - 1000 training examples (x10 different samples of 1000 training examples)
#        - 10000 training examples (x1 different sample of 10000 training examples)
# (because of jobman, the keys have to be strings, not ints)
# NOTE: Probably you don't want to make trainsize larger than 10K,
# because it will be too large for CPU memory.
validation_runs_for_each_trainingsize = {"100": 20, "1000": 10, "10000": 1}

# For each layer, a list of the epochs at which you evaluate the
# reconstruction error and linear-SVM-supervised error.
# All the different results you have from here will be stored in a
# separate file per layer.
#epochstest = [0,4,8,20,60,120]
epochstest = [4,8,20,60,120]

BATCH_TEST = 100
#BATCH_CREATION_LIBSVM = 500
BATCH_CREATION_LIBSVM = 1
NB_MAX_TRAINING_EXAMPLES_SVM = 10000

SVM_INITIALC    = 0.001
SVM_STEPFACTOR  = 10.
SVM_MAXSTEPS    = 10

#hardcoded path to your liblinear source:
SVMPATH = '/u/glorotxa/work/NLP/DARPAproject/netscale_sentiment_for_ET/lib/liblinear/'
#SVMPATH = '/home/turian/dev/python/DARPA-preprocessor/preprocessor_baseline_UdeM/lib/install/bin/'

#batchsize = 10
batchsize = 1

# The total number of files into which the training set is broken
nb_files = 15
#path_data = '/Tmp/glorotxa/'
#path_data = '/home/turian/dev/python/DeepANN/data/'
path_data = '/u/turian/dev/python/DeepANN-sparse/data/'
# Train and test (validation) here should be disjoint subsets of the
# original full training set.
name_traindata = 'OpenTable_5000_train_instances'
name_trainlabel =  'OpenTable_5000_train_labels'
name_testdata = 'OpenTable_5000_test_instances'
name_testlabel = 'OpenTable_5000_test_labels'

# If there is a model file specified to build upon, the output of this
# model is the input for the model we are currently building.
model_to_build_upon = None

ninputs = 5000

# inputtype ('binary', 'tfidf', other options?) determines what the
# decoding activation function is for the first layer
# e.g. inputtype 'tfidf' ('tf*idf'?) uses activation function softplus
# to decode the tf*idf.
inputtype = 'binary'

seed = 123
